{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating a Contextual Bandits Model\n",
    "\n",
    "Validation is straightforward in supervised learning because the \"ground truth\" is completely and unambiguously known. In the reinforcement learning case (RL), validation in inherently challenging due to the very nature of the problem: only partial \"ground truths\" are observed, or queried, from some unknown reward-generating process.<br><br>\n",
    "The following is one approach for validating contextual bandits models.\n",
    "\n",
    "## Historic Data\n",
    "The models used in Space Bandits benefit from direct reward approximation; given a set of features or a context, the model estimates an expected reward for each available action. This allows the model to optimize without direct access to the decision making policy used to query the reward-generating process.<br><br>\n",
    "The model directly regresses expected reward for each action based on a set of features. This makes regression metrics, such as RMSE, appropriate for evaluation. Due to the stochastic nature of the reward-generating process, we should not expect regression error metrics to be small. However, we would expect an optimized model to minimize such an error metric.\n",
    "## Naive Benchmark\n",
    "In the multi-arm bandit case, the expected reward for a given action can be approximated by computing the mean of observed rewards from this action. This special case provides a convenient <b>naive benchmark for the expected value of each action</b>, which we call $\\mathbb{E}_{b}[\\mathcal{A}]$.<br><br>\n",
    "We can use $\\mathbb{E}_{b}[\\mathcal{A}]$ to compute a benchmark error vector, $\\epsilon_{b}[\\mathcal{A}]$ for each action given a validation set by simpling using $\\mathbb{E}_{b}[\\mathcal{a}]$ as a <b>naive predicted reward</b> for a chosen action in the validation set and computing the RMSE against the observed reward, $\\mathcal{R}_{obs}$. \n",
    "$$\n",
    "\\epsilon_{b}[\\mathcal{A}] = \\sum_{n_{a}=0}^{N_{obs, a}}RMSE(\\mathbb{E}_{b}[\\mathcal{a}], \\mathcal{r}_{obs, n}),\n",
    "$$\n",
    "where $\\mathcal{r}_{obs, n}$ is the observed reward for validation example n.\n",
    "\n",
    "We define the model error vector as \n",
    "\n",
    "$$\n",
    "\\epsilon_{m}[\\mathcal{A}] = \\sum_{n_{a}=0}^{N_{obs, a}}RMSE(\\mathcal{r}_{pred,n}, \\mathcal{r}_{obs, n}),\n",
    "$$\n",
    "where $\\mathcal{r}_{pred, n}$ is the model's expected value of the reward for validation example n.\n",
    "\n",
    "\n",
    "This provides a benchmark with which to compare our model's RMSE, $\\epsilon_{m}[\\mathcal{A}]$ on the same prediction task on the validation set. If the condition $$\n",
    "\\sum_{a=0}^{A} \\frac{\\epsilon_{m}[\\mathcal{a}]}{\\epsilon_{b}[\\mathcal{a}]} < 1\n",
    "$$\n",
    "is met, we can be confident that our model is performing better than a simple multi-arm bandit model by conditioning on the context. For a simple \"higher-is-better\" score, we can define a contextual bandit model validation score $\\mathcal{S}$ as:\n",
    "$$\n",
    "\\mathcal{S} = \\sum_{a=0}^{A} 1 - \\frac{\\epsilon_{m}[\\mathcal{a}]}{\\epsilon_{b}[\\mathcal{a}]}\n",
    "$$\n",
    "\n",
    "Any value $\\mathcal{S} > 0$ is evidence for model convergence.\n",
    "\n",
    "## Example with Toy Data\n",
    "Using the same toy data used in the [toy problem notebook](toy_problem.ipynb), which we know  converges, we can compute S and show that, for the converged model, $\\mathcal{S} > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ARPU</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>62.009640</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.0</td>\n",
       "      <td>46.434857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.0</td>\n",
       "      <td>103.411680</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.0</td>\n",
       "      <td>92.682164</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.0</td>\n",
       "      <td>109.850754</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age        ARPU  action  reward\n",
       "0  19.0   62.009640       2       0\n",
       "1  44.0   46.434857       0       0\n",
       "2  30.0  103.411680       0      10\n",
       "3  23.0   92.682164       1       0\n",
       "4  19.0  109.850754       1       0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import random, randint\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "%config InlineBackend.figure_format='retina'\n",
    "##Generate Data\n",
    "\n",
    "from space_bandits.toy_problem import generate_dataframe\n",
    "\n",
    "df = generate_dataframe(4000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce a dataset with randomly selected actions and 4000 rows.\n",
    "## Train/Validation Split\n",
    "We split the data into two equally-sized groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.sample(frac=.5)\n",
    "val = df[~df.index.isin(train.index)]\n",
    "num_actions = len(train.action.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this for computing error metric\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\epsilon_{b}[\\mathcal{A}]$\n",
    "We use the train set to compute $\\mathbb{E}_{b}[\\mathcal{A}]$ to get the benchmark error vector, $\\epsilon_{b}[\\mathcal{A}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute benchmark expected value per action\n",
    "E_b = [train[train.action == a].reward.mean() for a in range(num_actions)]\n",
    "Err_b = []\n",
    "for a in range(num_actions):\n",
    "    slc = val[val.action == a]\n",
    "    y_pred = [E_b[a] for x in range(len(slc))]\n",
    "    y_true = slc.reward\n",
    "    error = mean_squared_error(y_pred, y_true)\n",
    "    Err_b.append(error)\n",
    "Err_b = np.array(Err_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24.34257772,  52.31057004, 935.44289322])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Err_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model\n",
    "We fit the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space_bandits import NeuralBandits\n",
    "\n",
    "model = NeuralBandits(num_actions, num_features=2, layer_sizes=[50,12], training_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural_model-bnn for 100 steps...\n"
     ]
    }
   ],
   "source": [
    "model.fit(train[['age', 'ARPU']], train['action'], train['reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\epsilon_{m}[\\mathcal{A}]$\n",
    "We use the train set and compute the model expected rewards for each example in our validation set to get the model error vector, $\\epsilon_{m}[\\mathcal{A}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ARPU</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>62.009640</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11.256291</td>\n",
       "      <td>4.328109</td>\n",
       "      <td>3.595610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.0</td>\n",
       "      <td>46.434857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.604247</td>\n",
       "      <td>2.501696</td>\n",
       "      <td>20.869746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.0</td>\n",
       "      <td>109.850754</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.536264</td>\n",
       "      <td>3.178807</td>\n",
       "      <td>-0.042348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35.0</td>\n",
       "      <td>26.936240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.332897</td>\n",
       "      <td>2.808232</td>\n",
       "      <td>21.452622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21.0</td>\n",
       "      <td>105.947998</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>9.119907</td>\n",
       "      <td>3.142819</td>\n",
       "      <td>0.376821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age        ARPU  action  reward          0         1          2\n",
       "0  19.0   62.009640       2       0  11.256291  4.328109   3.595610\n",
       "1  44.0   46.434857       0       0   2.604247  2.501696  20.869746\n",
       "4  19.0  109.850754       1       0   9.536264  3.178807  -0.042348\n",
       "5  35.0   26.936240       0       0   2.332897  2.808232  21.452622\n",
       "6  21.0  105.947998       0      10   9.119907  3.142819   0.376821"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_values = model.expected_values(val[['age', 'ARPU']].values, scale=True)\n",
    "pred = pd.DataFrame()\n",
    "for a, vals in enumerate(expected_values):\n",
    "    pred[a] = vals\n",
    "#expected reward values\n",
    "pred.index = val.index\n",
    "#add them to validation df\n",
    "val = pd.concat([val, pred], axis=1)\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute error vector\n",
    "Err_m = []\n",
    "for a in range(num_actions):\n",
    "    slc = val[val.action == a]\n",
    "    y_pred = slc[a]\n",
    "    y_true = slc.reward\n",
    "    error = mean_squared_error(y_pred, y_true)\n",
    "    Err_m.append(error)\n",
    "Err_m = np.array(Err_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 15.30096855,  53.23198686, 867.05886111])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Err_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\mathcal{S}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contextual bandits model score is:  0.427\n"
     ]
    }
   ],
   "source": [
    "S = (1 - Err_m/Err_b).sum()\n",
    "print('The contextual bandits model score is: ', round(S, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As expected the model (which we know converges) yields a contextual bandits score $\\mathcal{S}>0$, which is evidence of convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
