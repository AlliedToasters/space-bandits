{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating a Contextual Bandits Model\n",
    "\n",
    "Validation is straightforward in supervised learning because the \"ground truth\" is completely and unambiguously known. In the reinforcement learning case (RL), validation in inherently challenging due to the very nature of the problem: only partial \"ground truths\" are observed, or queried, from some unknown reward-generating process.<br><br>\n",
    "Here I propose an approach for validating contextual bandits models.\n",
    "\n",
    "## Historic Data\n",
    "The models used in Space Bandits benefit from direct reward approximation; given a set of features or a context, the model estimates an expected reward for each available action. This allows the model to optimize without direct access to the decision making policy used to query the reward-generating process.<br><br>\n",
    "The model directly regresses expected reward for each action based on a set of features. This makes regression metrics, such as RMSE, appropriate for evaluation. Due to the stochastic nature of the reward-generating process, we should not expect regression error metrics to be small. However, we would expect an optimized model to minimize such an error metric.\n",
    "## Naive Benchmark\n",
    "In the multi-arm bandit case, the expected reward for a given action can be approximated by computing the mean of observed rewards from this action. This special case provides a convenient <b>naive benchmark for the expected value of each action</b>, which we call $\\mathbb{E}_{b}[\\mathcal{A}]$.<br><br>\n",
    "We can use $\\mathbb{E}_{b}[\\mathcal{A}]$ to compute a benchmark error vector, $\\epsilon_{b}[\\mathcal{A}]$ for each action given a validation set by simpling using $\\mathbb{E}_{b}[\\mathcal{a}]$ as a <b>naive predicted reward</b> for a chosen action in the validation set and computing the RMSE against the observed reward, $\\mathcal{R}_{obs}$. \n",
    "$$\n",
    "\\epsilon_{b}[\\mathcal{A}] = \\sum_{n_{a}=0}^{N_{obs, a}}RMSE(\\mathbb{E}_{b}[\\mathcal{a}], \\mathcal{r}_{obs, n}),\n",
    "$$\n",
    "where $\\mathcal{r}_{obs, n}$ is the observed reward for validation example n.\n",
    "\n",
    "We define the model error vector as \n",
    "\n",
    "$$\n",
    "\\epsilon_{m}[\\mathcal{A}] = \\sum_{n_{a}=0}^{N_{obs, a}}RMSE(\\mathcal{r}_{pred,n}, \\mathcal{r}_{obs, n}),\n",
    "$$\n",
    "where $\\mathcal{r}_{pred, n}$ is the model's expected value of the reward for validation example n.\n",
    "\n",
    "\n",
    "This provides a benchmark with which to compare our model's RMSE, $\\epsilon_{m}[\\mathcal{A}]$ on the same prediction task on the validation set. If the condition $$\n",
    "\\sum_{a=0}^{A} \\frac{\\epsilon_{m}[\\mathcal{a}]}{\\epsilon_{b}[\\mathcal{a}]} < 1\n",
    "$$\n",
    "is met, we can be confident that our model is performing better than a simple multi-arm bandit model by conditioning on the context. For a simple \"higher-is-better\" score, we can define a contextual bandit model validation score $\\mathcal{S}$ as:\n",
    "$$\n",
    "\\mathcal{S} = \\sum_{a=0}^{A} 1 - \\frac{\\epsilon_{m}[\\mathcal{a}]}{\\epsilon_{b}[\\mathcal{a}]}\n",
    "$$\n",
    "\n",
    "Any value $\\mathcal{S} > 0$ is evidence for model convergence.\n",
    "\n",
    "## Example with Toy Data\n",
    "Using the same toy data used in the [toy problem notebook](toy_problem.ipynb), which we know  converges, we can compute S and show that, for the converged model, $\\mathcal{S} > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ARPU</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>92.067812</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.0</td>\n",
       "      <td>44.151515</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.0</td>\n",
       "      <td>50.710585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>59.794778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.0</td>\n",
       "      <td>53.120689</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age       ARPU  action  reward\n",
       "0  22.0  92.067812       2       0\n",
       "1  44.0  44.151515       2       0\n",
       "2  48.0  50.710585       0       0\n",
       "3  41.0  59.794778       1       0\n",
       "4  39.0  53.120689       2     100"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import random, randint\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "%config InlineBackend.figure_format='retina'\n",
    "##Generate Data\n",
    "\n",
    "def get_customer(ctype=None):\n",
    "    \"\"\"Customers come from two feature distributions.\n",
    "    Class 1: mean age 25, var 5 years, min age 18\n",
    "             mean ARPU 100, var 15\n",
    "    Class 2: mean age 45, var 6 years\n",
    "             mean ARPU 50, var 25\n",
    "    \"\"\"\n",
    "    if ctype is None:\n",
    "        if random() > .5: #coin toss\n",
    "            ctype = 1\n",
    "        else:\n",
    "            ctype = 2\n",
    "    age = 0\n",
    "    ft = -1\n",
    "    if ctype == 1:\n",
    "        while age < 18:\n",
    "            age = np.random.normal(25, 5)\n",
    "        while ft < 0:\n",
    "            ft = np.random.normal(100, 15)\n",
    "    if ctype == 2:\n",
    "        while age < 18:\n",
    "            age = np.random.normal(45, 6)\n",
    "        while ft < 0:\n",
    "            ft = np.random.normal(50, 25)\n",
    "    age = round(age)\n",
    "    return ctype, (age, ft)\n",
    "\n",
    "def get_rewards(customer):\n",
    "    \"\"\"\n",
    "    There are three actions:\n",
    "    promo 1: low value. 10 dollar if accept\n",
    "    promo 2: mid value. 25 dollar if accept\n",
    "    promo 3: high value. 100 dollar if accept\n",
    "    \n",
    "    Both groups are unlikely to accept promo 2.\n",
    "    Group 1 is more likely to accept promo 1.\n",
    "    Group 2 is slightly more likely to accept promo 3.\n",
    "    \n",
    "    The optimal choice for group 1 is promo 1; 90% acceptance for\n",
    "    an expected reward of 9 dollars each.\n",
    "    Group 2 accepts with 25% rate for expected 2.5 dollar reward\n",
    "    \n",
    "    The optimal choice for group 2 is promo 3; 20% acceptance for an expected\n",
    "    reward of 20 dollars each.\n",
    "    Group 1 accepts with 2% for expected reward of 2 dollars.\n",
    "    \n",
    "    The least optimal choice in all cases is promo 2; 10% acceptance rate for both groups\n",
    "    for an expected reward of 2.5 dollars.\n",
    "    \"\"\"\n",
    "    if customer[0] == 1: #group 1 customer\n",
    "        if random() > .1:\n",
    "            reward1 = 10\n",
    "        else:\n",
    "            reward1 = 0\n",
    "        if random() > .90:\n",
    "            reward2 = 25\n",
    "        else:\n",
    "            reward2 = 0\n",
    "        if random() > .98:\n",
    "            reward3 = 100\n",
    "        else:\n",
    "            reward3 = 0\n",
    "    if customer[0] == 2: #group 2 customer\n",
    "        if random() > .75:\n",
    "            reward1 = 10\n",
    "        else:\n",
    "            reward1 = 0\n",
    "        if random() > .90:\n",
    "            reward2 = 25\n",
    "        else:\n",
    "            reward2 = 0\n",
    "        if random() > .80:\n",
    "            reward3 = 100\n",
    "        else:\n",
    "            reward3 = 0\n",
    "    return np.array([reward1, reward2, reward3])\n",
    "\n",
    "def get_cust_reward():\n",
    "    \"\"\"returns a customer and reward vector\"\"\"\n",
    "    cust = get_customer()\n",
    "    reward = get_rewards(cust)\n",
    "    age = cust[1]\n",
    "    return np.array([age])/100, reward\n",
    "\n",
    "def generate_dataframe(n_rows):\n",
    "    df = pd.DataFrame()\n",
    "    ages = []\n",
    "    ARPUs = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    for i in range(n_rows):\n",
    "        cust = get_customer()\n",
    "        reward_vec = get_rewards(cust)\n",
    "        context = np.array([cust[1]])\n",
    "        ages.append(context[0, 0])\n",
    "        ARPUs.append(context[0, 1])\n",
    "        action = np.random.randint(0,3)\n",
    "        actions.append(action)\n",
    "        reward = reward_vec[action]\n",
    "        rewards.append(reward)\n",
    "\n",
    "    df['age'] = ages\n",
    "    df['ARPU'] = ARPUs\n",
    "    df['action'] = actions\n",
    "    df['reward'] = rewards\n",
    "\n",
    "    return df\n",
    "\n",
    "df = generate_dataframe(4000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce a dataset with randomly selected actions and 4000 rows.\n",
    "## Train/Validation Split\n",
    "We split the data into two equally-sized groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.sample(frac=.5)\n",
    "val = df[~df.index.isin(train.index)]\n",
    "num_actions = len(train.action.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\epsilon_{b}[\\mathcal{A}]$\n",
    "We use the train set to compute $\\mathbb{E}_{b}[\\mathcal{A}]$ to get the benchmark error vector, $\\epsilon_{b}[\\mathcal{A}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute benchmark expected value per action\n",
    "E_b = [train[train.action == a].reward.mean() for a in range(num_actions)]\n",
    "Err_b = []\n",
    "for a in range(num_actions):\n",
    "    slc = val[val.action == a]\n",
    "    y_pred = [E_b[a] for x in range(len(slc))]\n",
    "    y_true = slc.reward\n",
    "    error = mean_squared_error(y_pred, y_true)\n",
    "    Err_b.append(error)\n",
    "Err_b = np.array(Err_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model\n",
    "We fit the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model neural_model-bnn.\n"
     ]
    }
   ],
   "source": [
    "from space_bandits import NeuralBandits\n",
    "\n",
    "model = NeuralBandits(num_actions, num_features=2, layer_sizes=[50,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural_model-bnn for 100 steps...\n"
     ]
    }
   ],
   "source": [
    "model.fit(train[['age', 'ARPU']], train['action'], train['reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\epsilon_{m}[\\mathcal{A}]$\n",
    "We use the train set and compute the model expected rewards for each example in our validation set to get the model error vector, $\\epsilon_{m}[\\mathcal{A}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ARPU</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>59.794778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.325435</td>\n",
       "      <td>1.676436</td>\n",
       "      <td>10.027914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24.0</td>\n",
       "      <td>98.758904</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7.983790</td>\n",
       "      <td>1.388376</td>\n",
       "      <td>1.201940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40.0</td>\n",
       "      <td>37.461131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.199800</td>\n",
       "      <td>1.865051</td>\n",
       "      <td>23.888348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42.0</td>\n",
       "      <td>38.994888</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.541148</td>\n",
       "      <td>2.103595</td>\n",
       "      <td>22.969137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>21.0</td>\n",
       "      <td>103.160482</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10.148564</td>\n",
       "      <td>1.719807</td>\n",
       "      <td>1.448784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age        ARPU  action  reward          0         1          2\n",
       "3   41.0   59.794778       1       0   1.325435  1.676436  10.027914\n",
       "6   24.0   98.758904       0      10   7.983790  1.388376   1.201940\n",
       "8   40.0   37.461131       0       0   2.199800  1.865051  23.888348\n",
       "9   42.0   38.994888       1       0   1.541148  2.103595  22.969137\n",
       "10  21.0  103.160482       2       0  10.148564  1.719807   1.448784"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_values = model.expected_values(val[['age', 'ARPU']].values)\n",
    "pred = pd.DataFrame()\n",
    "for a, vals in enumerate(expected_values):\n",
    "    pred[a] = vals\n",
    "#expected reward values\n",
    "pred.index = val.index\n",
    "#add them to validation df\n",
    "val = pd.concat([val, pred], axis=1)\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute error vector\n",
    "Err_m = []\n",
    "for a in range(num_actions):\n",
    "    slc = val[val.action == a]\n",
    "    y_pred = slc[a]\n",
    "    y_true = slc.reward\n",
    "    error = mean_squared_error(y_pred, y_true)\n",
    "    Err_m.append(error)\n",
    "Err_m = np.array(Err_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\mathcal{S}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contextual bandits model score is:  0.244\n"
     ]
    }
   ],
   "source": [
    "S = (1 - Err_m/Err_b).sum()\n",
    "print('The contextual bandits model score is: ', round(S, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As expected the model (which we know converges) yields a contextual bandits score $\\mathcal{S}>0$, which is evidence of convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev1]",
   "language": "python",
   "name": "conda-env-dev1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
